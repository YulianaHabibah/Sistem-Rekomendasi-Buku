# -*- coding: utf-8 -*-
"""proyek akhir

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JNRgZKY6us-fdKU1V4z5N473NECJFeZw

### Proyek akhir: Membuat Sistem Rekomendasi Buku dengan menggunakan Metode Collaborative Filtering -Yuliana Habibah

## Penyiapan Data

### Import Library
"""

!pip install optuna

# Untuk Google Colab (hanya jika kamu benar-benar butuh unggah/download file)
from google.colab import files

# Modul standar
import os
import zipfile

# Data dan analisis
import pandas as pd
import numpy as np

# Visualisasi
import matplotlib.pyplot as plt

# Machine Learning
import tensorflow as tf
from tensorflow.keras import layers, models

# Evaluasi
from sklearn.metrics import mean_squared_error

# Hyperparameter Optimization
import optuna

"""### Unduh Dataset

Dataset diambil dari Kaggle, sehingga perlu mengunggah file JSON kredensial dari akun terlebih dahulu.

"""

# Upload kaggle.json

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}"'.format(
      name=fn))

# Ubah permission file
!chmod 600 /content/kaggle.json

# Setup Kaggle environment
os.environ['KAGGLE_CONFIG_DIR'] = "/content"

"""![Book Recommendation Dataset](https://i.postimg.cc/0Q4fcMDB/rsz-bookrecommendationdataset.jpg)

Informasi Dataset:

Jenis | Keterangan
--- | ---
Title | Book Recommendation Dataset
Source | [Kaggle](https://www.kaggle.com/arashnic/book-recommendation-dataset)
Maintainer | [MÃ¶bius](https://www.kaggle.com/arashnic)
License | [CC0: Public Domain](https://creativecommons.org/publicdomain/zero/1.0/)
Usability | 10.0
"""

# Download Dataset
!kaggle datasets download -d arashnic/book-recommendation-dataset

# melakukan ekstraksi file zip
local_zip = 'book-recommendation-dataset.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/content/book-recommendation-dataset/')
zip_ref.close()

# Menghapus berkas zip yang sudah tidak diperlukan
!rm book-recommendation-dataset.zip

"""## Data Understanding

Dataset ini terdiri dari tiga file CSV, yaitu Books.csv, Ratings.csv, dan Users.csv.
Selanjutnya, kita akan menggunakan `pandas` untuk menampilkan isi dari masing-masing file tersebut.
"""

# Load dataset

books = pd.read_csv('book-recommendation-dataset/Books.csv')
ratings = pd.read_csv('book-recommendation-dataset/Ratings.csv')
users = pd.read_csv('book-recommendation-dataset/Users.csv')

"""### Books

Berikut ini adalah isi dari `Books.csv`
"""

books

# Menampilkan informasi struktur data dari dataset Books
books.info()

"""Berdasarkan output di atas, file `Books.csv` berisi informasi mengenai buku dengan total 271.360 baris dan 8 kolom, yang mencakup antara lain:


- `ISBN` : berisi kode ISBN dari buku  
- `Book-Title` : berisi judul buku
- `Book-Author` : berisi penulis buku
- `Year-Of-Publication` : tahun terbit buku  
- `Publisher` : penerbit buku  
- `Image-URL-S` : URL menuju gambar buku berukuran kecil
- `Image-URL-M` : URL menuju gambar buku berukuran sedang
- `Image-URL-L` : URL menuju gambar buku berukuran besar

### Ratings

Berikut ini adalah isi dari berkas `Ratings.csv`
"""

ratings

# Mengelompokkan dan menghitung jumlah setiap nilai rating
ratings.groupby('Book-Rating').count()

rating_counter = ratings.groupby('Book-Rating').count()
plt.figure(figsize=(10,5))
plt.title('Jumlah Rating Buku yang Diberikan Pengguna')
plt.xlabel('Rating')
plt.ylabel('Jumlah Buku')
plt.bar(rating_counter.index, rating_counter['ISBN'])
plt.grid(True)
plt.show()

"""Visualisasi di atas menunjukkan bahwa data bersifat tidak seimbang, dengan sebagian besar pengguna memberikan rating 0.

"""

ratings.info()  # info struktur data ratings

ratings.describe().apply(lambda s: s.apply(lambda x: format(x, 'f')))  # statistik deskriptif dengan format angka lengkap

"""Berdasarkan output di atas, file `Ratings.csv` berisi data rating buku yang diberikan oleh para pengguna. Dataset ini terdiri dari 1.149.780 baris dan 3 kolom, yaitu:

 - `User-ID` : berisi ID unik pengguna
 - `ISBN` : berisi kode ISBN buku yang diberi rating oleh pengguna
 - `Book-Rating` : berisi nilai rating yang diberikan oleh pengguna berkisar antara 0-10

### Users

Berikut ini adalah isi dari `Users.csv`
"""

users

users.info()

users.describe()

"""Dari hasil output sebelumnya, file `Users.csv` berisi informasi tentang pengguna. Dataset ini memiliki 278.858 baris dan mencakup 3 kolom, yaitu:

- `User-ID` : berisi ID unik pengguna
- `Location` : berisi data lokasi pengguna
- `Age` : berisi data usia pengguna

## Data Preparation
Sebelum memasuki tahap pemodelan, data perlu diproses melalui tahap *data preparation* terlebih dahulu. Berikut ini merupakan langkah-langkah yang dilakukan dalam proses tersebut:

### Handling Imbalanced Data

Karena sebelumnya diketahui bahwa data rating bersifat tidak seimbang, pada tahap ini saya melakukan pembersihan dengan menghapus data yang memiliki rating 0.
"""

ratings.drop(ratings[ratings["Book-Rating"] == 0].index, inplace=True)

"""Berikut ini adalah jumlah data setelah di-drop"""

ratings.shape

ratings

rating_counter = ratings.groupby('Book-Rating').count()
plt.figure(figsize=(10,5))
plt.title('Jumlah Rating Buku yang Diberikan Pengguna')
plt.xlabel('Rating')
plt.ylabel('Jumlah Buku')
plt.bar(rating_counter.index, rating_counter['ISBN'])
plt.grid(True)
plt.show()

"""### Encoding Data

Encoding dilakukan untuk menyandikan `User-ID` dan `ISBN` ke dalam indeks integer
"""

# Mengubah userID menjadi list tanpa nilai yang sama
user_ids = ratings['User-ID'].unique().tolist()

# Melakukan encoding userID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}

# Melakukan proses encoding angka ke ke userID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}

# Mengubah ISBN menjadi list tanpa nilai yang sama
isbn_list = ratings['ISBN'].unique().tolist()

# Melakukan encoding ISBN
isbn_to_isbn_encoded = {x: i for i, x in enumerate(isbn_list)}

# Melakukan proses encoding angka ke ISBN
isbn_encoded_to_isbn = {i: x for i, x in enumerate(isbn_list)}

"""Setelah itu hasil dari encoding akan dimapping ke dataframe `ratings`"""

# Mapping userID ke dataframe user
ratings['user'] = ratings['User-ID'].map(user_to_user_encoded)

# Mapping userID ke dataframe user
ratings['book'] = ratings['ISBN'].map(isbn_to_isbn_encoded)

ratings

ratings.info()

"""### Randomize Dataset

Berikut merupakan proses pengacakan data untuk memastikan distribusi data menjadi acak.
"""

# Mengacak dataset
df = ratings.sample(frac=1, random_state=42)
df

"""### Data Standardization and Splitting

Setelah data diacak, dataset kemudian dibagi menjadi dua bagian: 80% untuk melatih model dan 20% untuk validasi.

Selain itu, nilai rating yang awalnya berada pada rentang 0 hingga 10 juga distandarisasi ke dalam rentang 0 hingga 1 guna mempermudah proses pelatihan model.
"""

# Menghitung jumlah pengguna
num_users = len(user_to_user_encoded)
print(num_users)

# Menghitung jumlah buku berdasarkan ISBN
num_isbn = len(isbn_encoded_to_isbn)
print(num_isbn)

# Mengonversi kolom rating ke tipe float
df['Book-Rating'] = df['Book-Rating'].values.astype(np.float32)

# Menentukan nilai rating minimum
min_rating = min(df['Book-Rating'])

# Menentukan nilai rating maksimum
max_rating = max(df['Book-Rating'])

# Menampilkan informasi jumlah user, jumlah ISBN, serta nilai rating minimum dan maksimum
print('Number of User: {}, Number of ISBN: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_isbn, min_rating, max_rating
))

# Membuat variabel x yang berisi pasangan user dan book
x = df[['user', 'book']].values  # fitur input

# Membuat variabel y yang berisi nilai rating yang telah dinormalisasi
y = df['Book-Rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values  # label output

# Membagi dataset menjadi 80% untuk pelatihan dan 20% untuk validasi
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],     # data training (fitur)
    x[train_indices:],     # data validasi (fitur)
    y[:train_indices],     # data training (label)
    y[train_indices:]      # data validasi (label)
)

# Menampilkan isi variabel x dan y
print(x, y)

"""## Modelling

### Membuat Kelas RecommenderNet

Untuk memperoleh hasil model yang optimal, proyek ini memanfaatkan library `optuna` guna melakukan *hyperparameter tuning*, khususnya dalam mencari nilai terbaik untuk `embedding_size`.

-0
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.metrics import mean_squared_error

class RecommenderNet(tf.keras.Model):
    def __init__(self, num_users, num_isbn, embedding_size, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.num_users = num_users
        self.num_isbn = num_isbn
        self.embedding_size = embedding_size
        self.user_embedding = layers.Embedding(
            num_users,
            embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.user_bias = layers.Embedding(num_users, 1)
        self.isbn_embedding = layers.Embedding(
            num_isbn,
            embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.isbn_bias = layers.Embedding(num_isbn, 1)

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])
        isbn_vector = self.isbn_embedding(inputs[:, 1])
        isbn_bias = self.isbn_bias(inputs[:, 1])

        dot_product = tf.tensordot(user_vector, isbn_vector, 2)

        x = dot_product + user_bias + isbn_bias

        return tf.nn.sigmoid(x)

"""##Hyperparameter

Untuk memperoleh performa model yang optimal, proyek ini memanfaatkan library *Optuna* dalam proses *hyperparameter tuning*, yaitu pencarian kombinasi nilai hyperparameter terbaik. Pada kasus ini, parameter yang disesuaikan adalah nilai `embedding_size`.
"""

import numpy as np

def objective(trial):
    tf.keras.backend.clear_session()
    model = RecommenderNet(num_users=num_users, num_isbn=num_isbn, embedding_size=trial.suggest_int('embedding_size', 1, 15))

    # model compile
    model.compile(
        loss = tf.keras.losses.BinaryCrossentropy(),
        optimizer = keras.optimizers.Adam(learning_rate=0.001),
        metrics=[tf.keras.metrics.RootMeanSquaredError()]
    )

    model.fit(
        x = x_train,
        y = y_train,
        batch_size=200,
        epochs = 1,
        validation_data = (x_val, y_val)
    )

    y_pred= model.predict(x_val)

    # Calculate RMSE manually since squared=False is not supported
    return np.sqrt(mean_squared_error(y_val, y_pred))

study = optuna.create_study(direction="minimize")
study.optimize(objective, n_trials=15, timeout=500)

print("Number of finished trials: {}".format(len(study.trials)))

print("Best trial:")
trial = study.best_trial

print("  Value: {}".format(trial.value))

print("  Params: ")
for key, value in trial.params.items():
    print("    {}: {}".format(key, value))

"""### Melatih Model"""

# Get the best hyperparameters from the Optuna study
best_embedding_size = study.best_params['embedding_size']

# Create and compile the model with the best hyperparameters
model = RecommenderNet(
    num_users=num_users,
    num_isbn=num_isbn,
    embedding_size=best_embedding_size
)

model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)


# Memulai melatih
history = model.fit(
    x = x_train,
    y = y_train,
    batch_size=64,
    epochs = 10,
    validation_data = (x_val, y_val)
)

"""## Evaluasi"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.grid(True)
plt.show()

"""Berdasarkan metrik tersebut, dapat disimpulkan bahwa model yang dikembangkan memiliki nilai Root Mean Squared Error (RMSE) sebesar 0,185.

## Mendapatkan Rekomendasi
"""

books_df = books
df = pd.read_csv('book-recommendation-dataset/Ratings.csv')

# Mengambil sample user
user_id = df['User-ID'].sample(1).iloc[0]
book_read_by_user = df[df['User-ID'] == user_id]

# Operator bitwise (~), bisa diketahui di sini https://docs.python.org/3/reference/expressions.html
book_not_read = books_df[~books_df['ISBN'].isin(book_read_by_user.ISBN.values)]['ISBN']
book_not_read = list(
    set(book_not_read)
    .intersection(set(isbn_to_isbn_encoded.keys()))
)

book_not_read = [[isbn_to_isbn_encoded.get(x)] for x in book_not_read]
user_encoder = user_to_user_encoded.get(user_id)
user_book_array = np.hstack(
    ([[user_encoder]] * len(book_not_read), book_not_read)
)

ratings = model.predict(user_book_array).flatten()

# Mengambil 10 indeks dengan rating tertinggi
top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_book_isbns = [
    isbn_encoded_to_isbn.get(book_not_read[x][0]) for x in top_ratings_indices
]

# Menampilkan rekomendasi buku untuk pengguna
print('Menampilkan rekomendasi untuk pengguna:', user_id)
print('=' * 27)
print('Daftar buku dengan rating tinggi dari pengguna')
print('----------------------------------------')

# Mengambil 5 buku dengan rating tertinggi yang pernah dibaca pengguna
top_book_user = (
    book_read_by_user.sort_values(
        by='Book-Rating',
        ascending=False
    )
    .head(5)
    .ISBN.values
)

book_df_rows = books_df[books_df['ISBN'].isin(top_book_user)]
for row in book_df_rows.itertuples():
    print(row._3, "-", row._2)

print('----------------------------------------')
print('10 Rekomendasi Buku Teratas')
print('----------------------------------------')

# Menampilkan 10 rekomendasi buku berdasarkan prediksi model
recommended_books = books_df[books_df['ISBN'].isin(recommended_book_isbns)]
for row in recommended_books.itertuples():
    print(row._3, "-", row._2)

"""## Penutup

Sebagai penutup, model rekomendasi buku telah berhasil dikembangkan dan diuji sehingga mampu menghasilkan daftar buku yang sesuai dengan preferensi pengguna. Model ini dapat menjadi dasar dalam pengembangan sistem rekomendasi berbasis machine learning yang dapat diimplementasikan lebih lanjut ke dalam bentuk aplikasi yang siap digunakan oleh masyarakat luas, seperti platform peminjaman buku digital, toko buku online, atau sistem manajemen perpustakaan.

Meskipun telah menunjukkan hasil yang cukup memuaskan, model ini masih memiliki ruang untuk ditingkatkan. Pengembangan selanjutnya dapat mencakup penambahan fitur baru seperti informasi genre, sinopsis, atau ulasan pengguna sebagai input tambahan untuk memperkaya konteks rekomendasi. Selain itu, penggunaan teknik yang lebih kompleks seperti *deep learning* atau *hybrid recommender systems* juga berpotensi meningkatkan akurasi dan relevansi hasil rekomendasi.

Dengan pengembangan berkelanjutan, diharapkan model ini tidak hanya memberikan hasil yang tepat sasaran, tetapi juga mampu menghadirkan pengalaman pengguna yang lebih personal dan bermanfaat.

"""